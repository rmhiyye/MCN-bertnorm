{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d23d02-ff97-40bf-b63f-56640a19ca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangye/miniconda3/envs/NEN/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# IMPORTS:\n",
    "####################################\n",
    "\n",
    "# External dependencies:\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from scipy.spatial.distance import cosine, euclidean, cdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Internal libraries:\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from os import listdir, write\n",
    "from os.path import isfile, join, splitext\n",
    "from data_loader import file_list_loader, norm_list_loader, mention2concept, encoder, Dataset\n",
    "from preprocess import id_combination, lowercaser_mentions\n",
    "from normalization import NeuralNetwork, cos_dist\n",
    "from inference import tokenize, inference\n",
    "import my_global\n",
    " \n",
    "my_global._init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a06d4df-2da3-4be2-8712-44c51969355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# LOADING DATA\n",
    "####################################\n",
    "\n",
    "train_file = file_list_loader('./dataset/train/train_file_list.txt')\n",
    "norm_list = norm_list_loader('./dataset/train/train_norm.txt')\n",
    "train_norm, train_span_split = mention2concept('./dataset/train/train_note', './dataset/train/train_norm', train_file, with_text = False)\n",
    "\n",
    "test_file = file_list_loader('./dataset/test/test_file_list.txt')\n",
    "test_norm, test_span_split = mention2concept('./dataset/test/test_note', './dataset/test/test_norm_cui_replaced_with_unk', test_file, with_text = False)\n",
    "\n",
    "####################################\n",
    "# PRE-PROCESSING\n",
    "####################################\n",
    "\n",
    "train_dict = id_combination(train_norm)\n",
    "train_dict = lowercaser_mentions(train_dict)\n",
    "\n",
    "test_dict = id_combination(test_norm)\n",
    "test_dict = lowercaser_mentions(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac617fc2-bcea-4650-8a2c-43bb1dc13543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# INITIALIZING\n",
    "################################################\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "my_global.set_value('device', device)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6370cf67-ba99-445c-81d9-33d719a4d098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mentions: 6684\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# LOADING EMBEDDING MODEL\n",
    "################################################\n",
    "\n",
    "model_name = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "embbed_size = 768\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_length = 20\n",
    "\n",
    "my_global.set_value('model', model)\n",
    "my_global.set_value('tokenizer', tokenizer)\n",
    "my_global.set_value('max_length', max_length)\n",
    "my_global.set_value('embbed_size', embbed_size)\n",
    "\n",
    "X_train, y_train = encoder(train_dict, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb28c647-df5a-43b5-ac74-1687755c2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# PREPARING DATA\n",
    "################################################\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe43de0-7ccf-4a70-bb48-cac6b7a77426",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# TRAINING\n",
    "################################################\n",
    "\n",
    "# fine-tuning layer (linear)\n",
    "basenorm = NeuralNetwork(embbed_size).to(device)\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 50\n",
    "optimizer = torch.optim.NAdam(basenorm.parameters(), lr=learning_rate)\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# loss function\n",
    "loss_fn = cos_dist\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acffda4a-54e7-4f2b-8950-e95e753c06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "model.train()\n",
    "basenorm.train()\n",
    "\n",
    "def checkpoint_loader(checkpoint):\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    batch_loss = checkpoint['loss']\n",
    "    return epoch, model, optimizer, batch_loss\n",
    "\n",
    "def train(from_checkpoint = False):\n",
    "    if from_checkpoint:\n",
    "        epoch = input(\"Please enter a number of epoch to load model: \")\n",
    "        checkpoint = torch.load(f'./checkpoint/epoch{epoch}_checkpoint.pt')\n",
    "        start_epoch, model, optimizer, batch_loss = checkpoint_loader(checkpoint)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        for X, y in train_dataloader: # both X and y contains n=batch_size tokenized mentions and labels respectively\n",
    "            batch_loss = None\n",
    "            for tokenized_mention, tokenized_label in zip(X, y):\n",
    "                tokenized_mention = tokenized_mention.to(device)\n",
    "                tokenized_label = tokenized_label.to(device)\n",
    "                pred = basenorm(model(tokenized_mention)[0][:,0]) # Taking last hidden state of the embedding model and piping it into a linear layer.\n",
    "                ground_truth = basenorm(model(tokenized_label)[0][:,0])\n",
    "                loss = loss_fn(pred, ground_truth) # Cosine similarity between embedding of mention and associated label.\n",
    "                if batch_loss == None:\n",
    "                    batch_loss = loss.reshape(1,1)\n",
    "                else:\n",
    "                    batch_loss = torch.cat((batch_loss, loss.reshape(1,1)), dim=1) # Appends current loss to all losses in batch\n",
    "\n",
    "            # Backpropagation\n",
    "            batch_loss = torch.mean(batch_loss) # Averages loss over the whole batch.\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # Check if the directory already exists\n",
    "            if not os.path.exists('./checkpoint'):\n",
    "            # If the directory does not exist, create it\n",
    "                os.makedirs('./checkpoint')\n",
    "            checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': batch_loss\n",
    "                        }\n",
    "            # Save the checkpoint\n",
    "            torch.save(checkpoint, f'./checkpoint/epoch{epoch}_checkpoint.pt')\n",
    "            print(f\"Fine-tuning: Epoch nÂ° {epoch}, loss = {batch_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02b9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting training\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5a1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# TEST\n",
    "################################################\n",
    "\n",
    "# load the model\n",
    "model.eval()\n",
    "basenorm.eval()\n",
    "epoch = input(\"Please enter a number of epoch to load model: \")\n",
    "checkpoint = torch.load(f'./checkpoint/epoch{epoch}_checkpoint.pt')\n",
    "start_epoch, model, optimizer, batch_loss = checkpoint_loader(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0e284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding ontology concept labels...\n",
      "Number of concepts in ontology: 6684\n",
      "Done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building embeddings from cui list: 0it [00:00, ?it/s]/mnt/data/yangye/MCN/inference.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_mention = torch.tensor(tokenize(dd_test[id]['mention']).to(device))\n",
      "Building embeddings from cui list: 6925it [00:51, 134.93it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (20,) into shape (768,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dd_predictions \u001b[39m=\u001b[39m inference(norm_list\u001b[39m=\u001b[39;49mnorm_list, basenorm\u001b[39m=\u001b[39;49mbasenorm, dd_test\u001b[39m=\u001b[39;49mtest_dict)\n",
      "File \u001b[0;32m/mnt/data/yangye/MCN/inference.py:57\u001b[0m, in \u001b[0;36minference\u001b[0;34m(norm_list, basenorm, dd_test)\u001b[0m\n\u001b[1;32m     55\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m cui \u001b[39min\u001b[39;00m cui_encode\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m---> 57\u001b[0m     CUIVectorMatrix[i] \u001b[39m=\u001b[39m cui_encode[cui]\n\u001b[1;32m     58\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mDistance matrix calculation...\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (20,) into shape (768,)"
     ]
    }
   ],
   "source": [
    "dd_predictions = inference(norm_list=norm_list, basenorm=basenorm, dd_test=test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0df54ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf03661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
